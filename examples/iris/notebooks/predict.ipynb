{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from noronha.tools.serving import LazyModelServer\n",
    "from noronha.tools.shortcuts import model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    clf_path = os.path.join(path, 'clf.pkl')\n",
    "    clf = joblib.load(clf_path)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, clf):\n",
    "    features = json.loads(x)\n",
    "    features = np.array(features).reshape(1, -1)\n",
    "    return clf.predict(features)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "from flask import Flask, request\n",
    "from werkzeug.serving import run_simple\n",
    "\n",
    "from noronha.common.constants import DateFmt, OnlineConst, Task\n",
    "from noronha.common.errors import NhaDataError, PrettyError, MisusageError\n",
    "from noronha.common.logging import LOG\n",
    "from noronha.common.parser import assert_json, assert_str, StructCleaner\n",
    "from noronha.db.depl import Deployment\n",
    "from noronha.tools.shortcuts import require_movers\n",
    "from noronha.tools.utils import load_proc_monitor, HistoryQueue\n",
    "from noronha.tools.serving import ModelServer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyModelServer(ModelServer):  # TODO: better logs and error messages that the end user can receive\n",
    "    \n",
    "    def __init__(self, predict_func, load_model_func, model_name: str = None, max_models: int = 100):\n",
    "        \n",
    "        assert callable(load_model_func)\n",
    "        super().__init__(predict_func=predict_func, enrich=False)\n",
    "        self._load_model_func = load_model_func\n",
    "        self._model_name = model_name  # TODO: if not provided, resolve by shortcut model_meta\n",
    "        self._max_models = max_models\n",
    "        self._loaded_models = {}\n",
    "        self._model_usage = HistoryQueue(max_size=max_models)\n",
    "    \n",
    "    def purge_model(self):\n",
    "        \n",
    "        least_used = self._model_usage.get()\n",
    "        _ = self._loaded_models.pop(least_used)\n",
    "    \n",
    "    def load_model(self, version):\n",
    "        \n",
    "        if len(self._loaded_models) >= self._max_models:\n",
    "            self.purge_model()\n",
    "            self.load_model(version)\n",
    "        else:  # TODO: before requiring, check if exists locally\n",
    "            path = require_movers(model=self._model_name, version=version)\n",
    "            self._loaded_models[version] = self._load_model_func(path)\n",
    "    \n",
    "    def fetch_model(self, version):\n",
    "        \n",
    "        if version not in self._loaded_models:\n",
    "            self.load_model(version)\n",
    "        \n",
    "        self._model_usage.put(version)\n",
    "        return self._loaded_models[version]\n",
    "        \n",
    "    def make_result(self, body, args):\n",
    "        \n",
    "        model = self.fetch_model(args['model_version'])\n",
    "        return self._predict_func(body, model)\n",
    "    \n",
    "    def make_metadata(self, body, args):\n",
    "        \n",
    "        raise MisusageError(\n",
    "            \"Inference metadata for {} is ambiguous\"\n",
    "            .format(self.__class__.__name__)\n",
    "        )\n",
    "    \n",
    "    def make_request_kwargs(self):\n",
    "        \n",
    "        body = request.get_data()\n",
    "        print(body)\n",
    "        charset = request.mimetype_params.get('charset') or OnlineConst.DEFAULT_CHARSET\n",
    "        print(charset)\n",
    "        print(dir(request))\n",
    "        try:\n",
    "            args = request.args\n",
    "            print(args)\n",
    "            print(type(args))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(repr(e))\n",
    "        return dict(\n",
    "            body=body.decode(charset, 'replace'),\n",
    "            args=request.args\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the prediction service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = LazyModelServer(\n",
    "    predict,\n",
    "    load,\n",
    "    model_name='iris-clf',\n",
    "    max_models=1\n",
    ")\n",
    "\n",
    "server()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
